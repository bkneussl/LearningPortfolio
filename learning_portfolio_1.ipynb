{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMhVz2VFeTRlWsBpyufE92K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkneussl/LearningPortfolio/blob/main/learning_portfolio_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1> Learning Portfolio**"
      ],
      "metadata": {
        "id": "2PlkmvxWxS6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 1 | The Machine Learning Landscape\n",
        "\n",
        "Use OpenAi's GPT-3 API and try to systematically analyze a dataset using an NLP task of your choice. Possible NLP tasks:\n",
        "\n",
        "1) Information search - try to find the right information by using several information texts and corresponding questions.\n",
        "\n",
        "2) Named entity recognition\n",
        "\n",
        "3) Text classification/Document Classification\n",
        "\n",
        "4) Keyword Extraction\n",
        "\n",
        "5) Grammatical Error Correction\n",
        "\n",
        "6) Text summarization\n",
        "\n",
        "7) Question Generation\n",
        "\n",
        "8) Headline Generation"
      ],
      "metadata": {
        "id": "4CracXdfx8Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Learnings**\n",
        "\n",
        "### **Zero Shot Learning vs Few-Shot Learning**\n",
        "\n",
        "**Zero-shot learning** and **few-shot learning** are both techniques used in machine learning and artificial intelligence that allow models to learn new concepts or objects with limited or no prior training. When it comes to Large Language Models (LLMs), such as GPT-3, there are some important differences between zero-shot and few-shot learning.\n",
        "\n",
        "**Zero-shot learning** in LLMs involves the ability to generate novel outputs for tasks that the model has never seen before, based on its pre-existing knowledge. For example, GPT-3 can be prompted to generate a poem about a topic it has never seen before by providing a prompt and context for the model to work with. This is possible because GPT-3 has been trained on a massive amount of text data, giving it a broad understanding of language and concepts.\n",
        "\n",
        "**Few-shot learning** in LLMs involves the ability to learn new tasks or concepts with only a few examples, rather than the thousands or millions of examples required in traditional machine learning. For example, GPT-3 can be fine-tuned on a few examples of a specific task, such as language translation, and then be able to perform that task more accurately. Few-shot learning is particularly useful in situations where there is limited data available or where it is not practical to collect large amounts of data for every new task.\n",
        "\n",
        "While both zero-shot learning and few-shot learning can be powerful tools for LLMs, there are some key differences between the two. Zero-shot learning is better suited for generating new outputs for tasks that the model has never seen before, while few-shot learning is better suited for learning new tasks with limited examples. Additionally, zero-shot learning relies heavily on the pre-existing knowledge of the model, while few-shot learning requires fine-tuning the model on specific examples. Therefore, depending on the task at hand, one technique may be more appropriate than the other.\n",
        "\n",
        "\n",
        "### **CO-HERE Client**\n",
        " \n",
        "In the first lecture, we explored Co:Here, a tool that offers API access to NLP models. A training set of phrases was provided to the model, and keyword extraction was attempted from this set. In this situation, the first occurrence of a humans first name in the sentence was the appropriate keyword to extract from the sentences.\n",
        "\n",
        "In addition, we imported a csv file with tweets regarding airline companies and tagged each one with an ordinal variable describing the perceived sentiment toward that airline. We constructed a training set using the sentence and the appropriate emotion, then attempted to predict all of the other sentences in the comma separated values file.\n",
        "\n",
        "<u>Parameter Description: </u>\n",
        "\n",
        "A basic prompt format that generally works well contains:\n",
        "*   A short description of the overall context\n",
        "*   A few examples of prompts and completions\n",
        "\n",
        "model: Either medium or xlarge. Generally, smaller models are faster while larger models will perform better.\n",
        "\n",
        "max_tokens - The maximum length of text to be generated. One word contains approximately 2-3 tokens.\n",
        "\n",
        "temperature - Ranges from 0 to 5. Controls the randomness of the output. Lower values tend to generate more “predictable” output, while higher values tend to generate more “creative” output. The sweet spot is typically between 0 and 1.\n",
        "\n",
        "stop_sequences - A stop sequence will cut off your generation at the end of the sequence. This effectively informs the model of when to stop. Add your stop sequence at the end of each example in the prompt (refer to the prompt we’d created, which uses “--” as the stop sequence).\n"
      ],
      "metadata": {
        "id": "UWNKHi_kRa8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere\n",
        "import cohere\n",
        "\n",
        "def cohere_text(prompt: str, tokens: int) -> str:\n",
        "  co = cohere.Client(apikey_cohere)\n",
        "\n",
        "  response = co.generate(  \n",
        "    model='xlarge',  \n",
        "    prompt = prompt,  \n",
        "    max_tokens=tokens,  \n",
        "    temperature=0.75,  \n",
        "    stop_sequences=[\"\\n\\n\"])\n",
        "\n",
        "  return response.generations[0].text"
      ],
      "metadata": {
        "id": "4qiFlzb0YpSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grammatical Error Correction"
      ],
      "metadata": {
        "id": "J5eeEgl3YzmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Define the GPT-3 prompt for grammar correction\n",
        "prompt = \"\"\"\n",
        "Correct the grammar in the following text:\n",
        "\n",
        "Text: \"TEXT\"\n",
        "\n",
        "Corrected Text:\n",
        "\"\"\"\n",
        "\n",
        "# function to perform grammar correction on a text using GPT-3\n",
        "def correct_grammar(text):\n",
        "    response = openai.Completion.create(\n",
        "        model=\"davinci\",\n",
        "        prompt=prompt.replace(\"TEXT\", text),\n",
        "        max_tokens=32,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    corrected_text = response.choices[0].text.strip()\n",
        "    return corrected_text\n",
        "\n",
        "# Define a piece of text to correct the grammar of\n",
        "text = \"I has a apple and two oranges.\"\n",
        "\n",
        "# Apply grammar correction to the text\n",
        "corrected_text = correct_grammar(text)\n",
        "\n",
        "# Output the results\n",
        "print(\"Original text:\\n\", text)\n",
        "print(\"Corrected text:\\n\", corrected_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCZ6Re9ZFm9-",
        "outputId": "be90b187-d084-402f-c8d1-f2f6d1a15327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Original text:\n",
            " I has a apple and two oranges.\n",
            "Corrected text:\n",
            " \"I have an apple and two oranges.\"\n",
            "\n",
            "\n",
            "\n",
            "The correct form is \"I have a apple and two oranges.\"\n",
            "\n",
            "The possessive form\n"
          ]
        }
      ]
    }
  ]
}